# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EqGHKAypv7CV77edOZXFaG1wVrMT6mAk
"""

import warnings
from tqdm import tqdm
warnings.filterwarnings('ignore')
import pickle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



# AS WE ONLY HAVE TO WORK WITH REVIEWS HAVING BUSINESS RESTAURANT BUSINESS AND ARE PRESENT IN BUSINESS DATASET 
# EXTRACT THE REQUIRED SUBSET OF DATA we have already cleaned it in baseline results
buss_data=pd.read_csv('/content/drive/MyDrive/business_final.csv')
review_data=pd.read_csv('/content/drive/MyDrive/yelp_review_final.csv')

buss_data.columns

review_data.columns

# update 'postal_code' column in the 'business' dataset as string object type
buss_data['postal_code'] = buss_data.postal_code.astype('str')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # shuffle the index of review_data for random split
# np.random.seed(42)
# idx = np.array(review_data.index, dtype='int')
# np.random.shuffle(idx)
# 
# # train-test-split by 87%-13% ratio
# train = review_data.loc[idx[:int(0.87*len(idx))]]
# test = review_data.loc[idx[int(0.87*len(idx)):]]
# #print("current train:test ratio:", len(train)/len(test))

""" Now we need to move reviews of user_id that only exists in testset  to trainset"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # add one review of user_id that only exists in testset back to trainset
# 
# # determine the idx to move from testset to trainset
# user_test_only = test[~test.user_id.isin(train.user_id.unique())] # filter testset to reviews by user_id only exists in the testset
# user_idx_toadd = user_test_only['user_id'].drop_duplicates().index # extract the index of the reviews to add back to the trainset
# 
# # update the new list of indices for the trainset and testset
# idx_train = train.index.union(user_idx_toadd)
# idx_test = review_data.index.difference(idx_train)
# 
# # update trainset and testset
# train = review_data.loc[idx_train]
# test = review_data.loc[idx_test]
# print("current train:test ratio:", len(train)/len(test))

# calculate the final train vs test sample ratio
print('final train/test ratio:', len(train)/len(test))
assert (len(train) + len(test)) == len(review_data)

"""**Building restaurant and user feature vectors from restaurant reviews**"""

# groupby business_id and concatenate all reviews of the same business together, reviews are separated by '###'
rev_by_rest = train.groupby('business_id').agg({'review_id': 'count', 'text': lambda i: '###'.join(i)}).rename(columns={'review_id':'review_count','text':'review_combined'})
rev_by_rest = rev_by_rest.reset_index()
rev_by_rest.head(10)

"""**To extract features we use tf-idf for reviews**"""

# Tfidf to extract top 1000 features from restaurant reviews
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1000) # limit to top 1000 words
X = vectorizer.fit_transform(rev_by_rest.review_combined)
print(vectorizer.get_feature_names_out()[:50]) # inspect the top 50 features



# convert it into a dataframe and look at bigrams picked in the dataset
rest_revfeature = pd.DataFrame.sparse.from_spmatrix(X)
rest_revfeature = rest_revfeature.set_index(rev_by_rest.business_id)
rest_revfeature.columns = vectorizer.get_feature_names_out()
rest_revfeature = rest_revfeature.fillna(0)

# look at all the bigrams being picked up in the top 1000 features
for i in rest_revfeature.columns:
    if len(i.split())>1:
        print(i, end=',')

X = None
vectorizer = None
rev_by_rest = None

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # feature selection
# from sklearn.decomposition import PCA
# pca = PCA()
# gh=rest_revfeature.sparse.to_dense()
# 
# rest_pcafeature = pca.fit_transform(gh)
# vr = pca.explained_variance_ratio_

# compute cumulative explained variance ratio
vr_cum = [sum(vr[:i+1]) for i in range(len(vr))]
# plot explained variance ratio vs the number of PCA components
plt.plot(list(range(len(vr))),vr_cum, color='blue');

# extract and inspect the top 6 PCA components, in relationship to the original review features
components = pd.DataFrame(data=pca.components_, columns = rest_revfeature.columns)
for i in range(6):
    component = components.loc[i].sort_values(ascending=False)
    print("principle component #{}:\n".format(i), component[:5])

# keep only the top 300 PCA components
rest_pcafeature = pd.DataFrame(rest_pcafeature[:,:300], index=rest_revfeature.index, columns=[str(i) for i in np.arange(1,301)])
rest_pcafeature.columns.name = 'pca_components'

# rescale rest_pcafeature coefficients so all restaurant feature vectors share the same unit length of 1
rest_pcafeature['root_squared_sum'] = rest_pcafeature.apply(lambda row: np.sqrt(sum([i*i for i in row])), axis=1)
rest_pcafeature = rest_pcafeature.drop('root_squared_sum',axis=1).divide(rest_pcafeature.root_squared_sum, axis=0)
rest_pcafeature.head(10)

"""**Compute user feature vector**
 build user preference profile by compute a weighted sum of the restaurant vectors for all rated items by the user
"""

user_pcafeature = pd.merge(train[['user_id','business_id','stars']], rest_pcafeature, how='inner',left_on='business_id',right_index=True)

user_pcafeature

user_pcafeature=user_pcafeature.drop('business_id', axis=1)

# rescale component coefficients by the review rating as the correpsonding weight
user_pcafeature.loc[:, '1':'300'] = user_pcafeature.loc[:,'1':'300'].multiply(user_pcafeature.stars, axis=0)
# sum up all component coefficients by user_id
user_pcafeature = user_pcafeature.drop('stars', axis=1).groupby('user_id').sum()

# rescale user_pcafeature coefficients so all user feature vectors share the same unit length of 1
user_pcafeature['root_squared_sum'] = user_pcafeature.apply(lambda row: np.sqrt(sum([i*i for i in row])), axis=1)
user_pcafeature = user_pcafeature.drop('root_squared_sum', axis=1).divide(user_pcafeature.root_squared_sum, axis=0)

"""**Save restaurant and user pca features to file**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # save rest_pcafeature as pickled file
# with open('rest_pcafeature_train.pkl','wb') as f:
#     pickle.dump(rest_pcafeature, f)
#     
# # save user_pcafeature as pickled file, note: need to write in chucks of size smaller than 2GB due to a bug in Python3
# max_bytes = 2**31 - 1
# bytes_out = pickle.dumps(user_pcafeature)
# with open('user_pcafeature_train.pkl','wb') as f:
#     for idx in range(0, len(bytes_out), max_bytes):
#         f.write(bytes_out[idx:idx+max_bytes])
# 
# rest_revfeature = None
# components = None
# vr = None
# vr_cum = None
# pca = None
# rest_pcafeature = None
# user_pcafeature = None

"""**Building restaurant and user feature vector from restaurant metadata**

There are two types of metadata in the 'business' dataset:

**numerical metadata:**
columns include 'latitude','longitude', 'stars' and 'review_count'.
**2) text-based metadata:**
all other columns contain some types of text-based information, either nested dictionaries, or categorical values, or True or False booleans. 

**Note:** text-based metadata are extracted and used here to build restaurant and user feature vectorse, whereas the numerical metadata are separated for use later in the supervised regression model.

**Transform text-based metadata of restaurants**
"""

buss_data.columns

rest_metadata = buss_data[['business_id','postal_code','city','state','name','categories']]

buss_data.address.head(1)
buss_data= buss_data.drop('address', axis=1)

"""**TRANSFORM THE TEXT ATTRIBUTES BASED DATA**"""

# transform the 'hours' column (and all child columns named 'hours.*')

def hours_transform(row):
    """function to transform and extract information from the 'hours' column (and all child columns named 'hours.*')
    of the 'business' dataframe"""
    row.hours = ''
    for day in ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']:
        col = 'hours.{}'.format(day)
        if row[col] is not np.nan:
            row.hours += day + ','
    if len(row.hours) > 0:
        row.hours = row.hours[:-1] # trim off the extra comma ',' at the end
    return row

# apply the transform and add the transformed column to rest_metafeature
rest_metadata['hours'] = buss_data.apply(hours_transform, axis=1).hours
rest_metadata.hours.head(2)

#  at (bring your own bottle) and 'Corkage'  related columns
print(len(buss_data[['attributes.BYOBCorkage','attributes.BYOB','attributes.Corkage']].dropna(how='all')))
buss_data.groupby(['attributes.BYOBCorkage','attributes.BYOB','attributes.Corkage'])['business_id'].count()

"""As shown, data in these three columns are not consistent, and there are therefore information in these columns are not incorporated for analysis."""

# transform the child columns (named 'attributes.*') with categorical values

def attributes_transform_cate(row):
    row.attributes=''
    
    # names of the child columns to be transformed
    cat_cols = ['WheelchairAccessible','HappyHour','Alcohol','NoiseLevel','HasTV','RestaurantsPriceRange2','CoatCheck',\
               'Open24Hours','RestaurantsReservations','GoodForDancing','WiFi','RestaurantsAttire','RestaurantsDelivery',\
               'RestaurantsTakeOut','Smoking','BusinessAcceptsCreditCards','BusinessAcceptsBitcoin','GoodForKids',\
                'AgesAllowed','Caters','DogsAllowed','BikeParking','OutdoorSeating','RestaurantsGoodForGroups',\
               'RestaurantsCounterService','ByAppointmentOnly','RestaurantsTableService','DriveThru']
    
    # transform all child columns with categorical values (binary, or categorical)
    for attr in cat_cols:
        col = 'attributes.{}'.format(attr)
        if row[col] is not np.nan:
            row.attributes += attr + '_' + str(row[col]) + ','  

    if len(row.attributes) > 0:
        row.attributes = row.attributes[:-1]
    
    return row

# apply the transform and add the transformed column as 'attr_cate' to dataframe 'rest_metadata'
rest_metadata['attr_cate'] = buss_data.apply(attributes_transform_cate, axis=1).attributes

rest_metadata.attr_cate

#transform the child columns (named 'attributes.*') with nested dictionaries

def convert_to_dict(s):
    """convert a non-null string of special format (invalid JSON format) to a dictionary.
    """
    a = [i.strip() for i in s[1:-1].split(',')] # extract content between '{' and '}' and split by comma ','
    b = [i.split(':') for i in a]
    for i in b: 
        i[0] = i[0].strip('\' ')
        
        i[1] = bool(i[1].strip())
    
    return dict(b)

def attributes_transform_dict(row):
    row.attributes=''
    
    # names of the child columns to be transformed
    dict_cols = ['DietaryRestrictions','GoodForMeal','BusinessParking','BestNights','Music','Ambience']
    
    # transform child columns with nested dictionaries
    for attr in dict_cols:
        col = 'attributes.{}'.format(attr)
        if row[col] is not np.nan:
            row[col] = convert_to_dict(row[col])
            for k,v in row[col].items():
                if v is not np.nan:
                    row.attributes += attr + '_' + k + '_' + str(v) + ',' 

    if len(row.attributes) > 0:
        row.attributes = row.attributes[:-1]
    
    return row

#apply the transform and add the transformed column as 'attr_dict' to dataframe 'rest_metadata'
#rest_metadata['attr_dict'] = buss_data.apply(attributes_transform_dict, axis=1).attributes

rest_metadata = rest_metadata.set_index('business_id') # set 'business_id' as the index col
rest_metadata.info()

# combine all columns into a single column of all text features
rest_metadata['metafeature'] = rest_metadata.apply(lambda row: ','.join([str(i) for i in row if i is not np.nan]), axis=1)
rest_metadata.metafeature.head(2)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.feature_extraction.text import CountVectorizer
# vectorizer = CountVectorizer(stop_words=['the','and','in','of','at','by','city','el'],lowercase=True, max_features=300)
# X = vectorizer.fit_transform(rest_metadata.metafeature)
# print(vectorizer.get_feature_names_out()[:100]) # inspect the top 100 features

rest_metafeature = pd.DataFrame.sparse.from_spmatrix(X)
rest_metafeature = rest_metafeature.set_index(rest_metadata.index)
rest_metafeature.columns = vectorizer.get_feature_names_out()
rest_metafeature = rest_metafeature.fillna(0)

# rescale rest_metafeature coefficients so all restaurant feature vectors share the same unit length of 1
rest_metafeature['root_squared_sum'] = rest_metafeature.apply(lambda row: np.sqrt(sum([i*i for i in row])), axis=1)
rest_metafeature = rest_metafeature.drop('root_squared_sum',axis=1).divide(rest_metafeature.root_squared_sum, axis=0)
rest_metafeature.head(1)

# free up temporary variables to save memory use
X = None
vectorizer = None
rest_metadata = None

# save rest_metafeature as pickled file
with open('rest_metafeature_train.pkl','wb') as f:
    pickle.dump(rest_metafeature, f)

"""** User feature vector based on user ratings of restaurants**"""

# build user preference profile by compute a weighted sum of the restaurant vectors for all rated items by the user, with weights being the user’s rating
user_metafeature = pd.merge(train[['user_id','business_id','stars']], rest_metafeature, how='inner',left_on='business_id',right_index=True).drop('business_id',axis=1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # rescale component coefficients by the review rating as the correpsonding weight
# user_metafeature.loc[:, '19103':'yogurt'] = user_metafeature.loc[:,'19103':'yogurt'].multiply(user_metafeature.stars, axis=0)







# save rest_metafeature as pickled file
with open('rest_metafeature_train.pkl','wb') as f:
    pickle.dump(rest_metafeature, f)

# save user_metafeature as pickled file, note: need to write in chucks of size smaller than 2GB due to a bug in Python3
max_bytes = 2**31 - 1
bytes_out = pickle.dumps(user_metafeature)
with open('user_metafeature_train.pkl','wb') as f:
    for idx in range(0, len(bytes_out), max_bytes):
        f.write(bytes_out[idx:idx+max_bytes])

# free up temporary variables to save memory use
rest_metafeature = None
user_metafeature = None

from google.colab import files
files.download('/content/user_metafeature_train.pkl')

!zip -r /content/user_metafeature_train.zip /content/user_metafeature_train.pkl

with open('/content/rest_metafeature_train.pkl', 'rb') as f1:
    data1 = pickle.load(f1)

with open('/content/user_metafeature_train.pkl', 'rb') as f2:
    data2 = pickle.load(f2)

data1 = np.nan_to_num(data1, nan=0)
data2 = np.nan_to_num(data2, nan=0)


# compute cosine similarity matrix

from sklearn.metrics.pairwise import cosine_similarity

#similarity_matrix = cosine_similarity(data1, data2)

with open('similarity_meta_train.pkl', 'wb') as f:
pickle.dump(similarity_matrix, f)



"""**Rating prediction based on cosine similarity scores and restaurant numerical metadata**"""

# restaurant and user metafeature vector developed from restaurant text metadata in section #4 are used to compute pairwise cosine similarity scores (dot product of the restaurant feature vector and user feature vector)
# this similarity scores have been computed for all user-restaurant combinations available in the restaurant review dataset ('review_s')
# the actual computation was completed in a separate notebook, the similarity score along with the corresponding review_id are saved to a pickle file named 'similarity_meta_train.pkl'

with open('similarity_meta_train.pkl','rb') as f: # load in the computed cosine similarity score
    sim_meta_train = pickle.load(f)

sim_meta_train = sim_meta_train.set_index('review_id')

# combine all features

# extract numerical columns from business
busi_num = buss_data[['business_id','latitude','longitude','stars','review_count']]

# extract only the necessary information for reviews in the train and test set
train_2 = train[['review_id','user_id','business_id','stars']].set_index('review_id')
test_2 = test[['review_id','user_id','business_id','stars']].set_index('review_id')

# merge the similarity scores from restaurant text-based metadata with reviews in the train and test set separately
train_2 = train_2.join(sim_meta_train).join(sim_pca_train) # join on index ('review_id')
test_2 = test_2.join(sim_meta_train).join(sim_pca_train) # join on index ('review_id')

# merge restaurant numerical metadata with reviews in the train and test set separately
train_2 = train_2.merge(busi_num, how='inner', on='business_id', suffixes=('_review', '_busi')) # join on 'business_id'
test_2 = test_2.merge(busi_num, how='inner', on='business_id', suffixes=('_review', '_busi')) # join on 'business_id'

#Various linear and non-linear regression models are prototyped for predicting review ratings.

rom sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

# import regression models and metrics
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor 
from sklearn.metrics import r2_score, mean_squared_error 

# initiate the score table and define function for logging performance for regression problem
index = ['Lasso','Ridge','RandomForestRegressor','GradientBoostingRegressor']
score_table = pd.DataFrame(index = index, columns= ['r2_train','mse_train','rmse_train','r2_test','mse_test','rmse_test'])

# define function for logging the results
def compute_log_result(algo, pred_train, pred_test):
    """compute and log the performance into the score_table for both training and test sets"""
    
    # compute the performance
    r2_train = r2_score(y_train, pred_train)
    r2_test = r2_score(y_test, pred_test)
    mse_train = mean_squared_error(y_train, pred_train)
    mse_test = mean_squared_error(y_test, pred_test)
    rmse_train = np.sqrt(mse_train)
    rmse_test = np.sqrt(mse_test)
    
    # log the performance
    score_table.loc[algo,:] = r2_train, mse_train, rmse_train, r2_test, mse_test, rmse_test

    # extract feature and target variables
X_train, y_train = train_1.drop(['user_id','business_id','stars_review'], axis=1), train_1.stars_review
X_test, y_test = test_1.drop(['user_id','business_id','stars_review'], axis=1), test_1.stars_review

# fit Lasso regression model with optimized parameters
lasso = Pipeline([('scaler', StandardScaler()),('lasso', Lasso(alpha=0.0015, max_iter=1000, selection='random'))])
lasso.fit(X_train, y_train)
pred_train = lasso.predict(X_train)
pred_test = lasso.predict(X_test)
# print features oefficients 
feature_coef = pd.DataFrame({'feature':X_train.columns, 'coefficient':lasso.named_steps.lasso.coef_})
print("feature coefficients for the fitted Lasso model:\n",feature_coef.sort_values('coefficient',ascending=False))
# logging of model performance
compute_log_result("Lasso", pred_train, pred_test)

# fit Ridge regression model with optimized parameters
ridge = Pipeline([('scaler', StandardScaler()),('ridge',Ridge(alpha=100,max_iter=1000,tol=0.001))])
ridge.fit(X_train, y_train)
pred_train = ridge.predict(X_train)
pred_test = ridge.predict(X_test)
# print features and coefficients
feature_coef = pd.DataFrame({'feature':X_train.columns, 'coefficient':ridge.named_steps.ridge.coef_})
print("feature coefficients for the fitted Ridge model:\n", feature_coef.sort_values('coefficient',ascending=False))
# logging of model performance
compute_log_result("Ridge", pred_train, pred_test)

# fit random forest regression model with optimized parameters
rfr = Pipeline([('scaler', StandardScaler()),('rfr', RandomForestRegressor(n_estimators=70, max_features='log2'))])
rfr.fit(X_train, y_train)
pred_train = rfr.predict(X_train)
pred_test = rfr.predict(X_test)
# print feature importance
feature_rank = pd.DataFrame({'feature': X_train.columns, 'importance': rfr.named_steps.rfr.feature_importances_})
print("feature importance for the fitted Random Forest Regressor model:\n", feature_rank.sort_values(by='importance',ascending=False))
# logging of model performance
compute_log_result("RandomForestRegressor", pred_train, pred_test)

# fit gradient boosting regression model with optimized parameters
gbr = Pipeline([('scaler', StandardScaler()),('gbr', GradientBoostingRegressor(n_estimators=400, max_features='log2'))])
gbr.fit(X_train, y_train)
pred_train = gbr.predict(X_train)
pred_test = gbr.predict(X_test)
# print feature importance
feature_rank = pd.DataFrame({'feature': X_train.columns, 'importance': gbr.named_steps.gbr.feature_importances_})
print("feature importance for the fitted Gradient Boosting Regressor model:\n", feature_rank.sort_values(by='importance',ascending=False))
# logging of model performance
compute_log_result("GradientBoostingRegressor", pred_train, pred_test)

score_table # print the performance table for all tested algorithms

#As discussed above, two similarity scores are available:

#1.cosine similarity scores computed based on user and restaurant feature vectors extracted from retaurant text-type metadata using count vectorizer
#2.cosine similarity scores computed based on user and restaurant feature vectors extracted from restaurant reviews using Tfidf vectorizer

#In addition to the features crafted in #5.1 using restaurant numerical metadata and similarity scores from restaurant text metadata, simiarity scores from restaurant reviews are also added as an engineered feature representing personal preference.
#Therefore, not only are the entire restaurant metadata incorporated, but also the rich set of restaurant reviews. This enable the model to take advantage of all the available content-based information for personalized rating prediction.

# restaurant and user pcafeature vector developed from restaurant reviews in section #3 are used to compute pairwise cosine similarity scores (dot product of the restaurant feature vector and user feature vector)
# this similarity scores have been computed for all user-restaurant combinations available in the restaurant review dataset ('review_s')
# the actual computation was completed in a separate notebook, the similarity score along with the corresponding review_id are saved to a pickle file named 'similarity_pca_train.pkl'

with open('similarity_pca_train.pkl','rb') as f: # load in the computed similarity score
    sim_pca_train = pickle.load(f)

sim_pca_train = sim_pca_train.set_index('review_id')

# combine all features

# extract numerical columns from business
busi_num = business[['business_id','latitude','longitude','stars','review_count']]

# extract only the necessary information for reviews in the train and test set
train_2 = train[['review_id','user_id','business_id','stars']].set_index('review_id')
test_2 = test[['review_id','user_id','business_id','stars']].set_index('review_id')

# merge the similarity scores from restaurant text-based metadata with reviews in the train and test set separately
train_2 = train_2.join(sim_meta_train).join(sim_pca_train) # join on index ('review_id')
test_2 = test_2.join(sim_meta_train).join(sim_pca_train) # join on index ('review_id')

# merge restaurant numerical metadata with reviews in the train and test set separately
train_2 = train_2.merge(busi_num, how='inner', on='business_id', suffixes=('_review', '_busi')) # join on 'business_id'
test_2 = test_2.merge(busi_num, how='inner', on='business_id', suffixes=('_review', '_busi')) # join on 'business_id'

#Regression models for rating prediction based on the engineered features

 # extract feature and target variables
X_train, y_train = train_2.drop(['user_id','business_id','stars_review'], axis=1), train_2.stars_review
X_test, y_test = test_2.drop(['user_id','business_id','stars_review'], axis=1), test_2.stars_review

# fit Lasso regression model with optimized parameters
lasso = Pipeline([('scaler', StandardScaler()),('lasso', Lasso(alpha=0.003, max_iter=1000, selection='random'))])
lasso.fit(X_train, y_train)
pred_train = lasso.predict(X_train)
pred_test = lasso.predict(X_test)
# print features oefficients 
feature_coef = pd.DataFrame({'feature':X_train.columns, 'coefficient':lasso.named_steps.lasso.coef_})
print("feature coefficients for the fitted Lasso model:\n",feature_coef.sort_values('coefficient',ascending=False))
# logging of model performance
compute_log_result("Lasso", pred_train, pred_test)

# fit Ridge regression model with optimized parameters
ridge = Pipeline([('scaler', StandardScaler()),('ridge',Ridge(alpha=30000,max_iter=1000,tol=0.001))])
ridge.fit(X_train, y_train)
pred_train = ridge.predict(X_train)
pred_test = ridge.predict(X_test)
# print features and coefficients
feature_coef = pd.DataFrame({'feature':X_train.columns, 'coefficient':ridge.named_steps.ridge.coef_})
print("feature coefficients for the fitted Ridge model:\n", feature_coef.sort_values('coefficient',ascending=False))
# logging of model performance
compute_log_result("Ridge", pred_train, pred_test)



# fit random forest regression model with optimized parameters
rfr = Pipeline([('scaler', StandardScaler()),('rfr', RandomForestRegressor(n_estimators=70, max_features='log2'))])
rfr.fit(X_train, y_train)
pred_train = rfr.predict(X_train)
pred_test = rfr.predict(X_test)
# print feature importance
feature_rank = pd.DataFrame({'feature': X_train.columns, 'importance': rfr.named_steps.rfr.feature_importances_})
print("feature importance for the fitted Random Forest Regressor model:\n", feature_rank.sort_values(by='importance',ascending=False))
# logging of model performance
compute_log_result("RandomForestRegressor", pred_train, pred_test)

# fit gradient boosting regression model with optimized parameters
gbr = Pipeline([('scaler', StandardScaler()),('gbr', GradientBoostingRegressor(n_estimators=700, max_features='auto'))])
gbr.fit(X_train, y_train)
pred_train = gbr.predict(X_train)
pred_test = gbr.predict(X_test)
# print feature importance
feature_rank = pd.DataFrame({'feature': X_train.columns, 'importance': gbr.named_steps.gbr.feature_importances_})
print("feature importance for the fitted Gradient Boosting Regressor model:\n", feature_rank.sort_values(by='importance',ascending=False))
# logging of model performance
compute_log_result("GradientBoostingRegressor", pred_train, pred_test)

score_table # print the performance table for all tested algorithms

#Update the predicted ratings for the testset using the best performing regression model

# use the best supervised regression model to predict rating for the testset

busi_num = business[['business_id','latitude','longitude','stars','review_count']]
train_1 = train[['review_id','user_id','business_id','stars']].set_index('review_id')
test_1 = test[['review_id','user_id','business_id','stars']].set_index('review_id')

train_1 = train_1.join(sim_meta_train) # join 'train' and 'sim_pca_train' on index ('review_id')
test_1 = test_1.join(sim_meta_train) # join 'test' and 'sim_pca_train' on index ('review_id')
train_1 = train_1.merge(busi_num, how='inner', on='business_id', suffixes=('_review', '_busi')) # join 'train' and 'busi_num' on 'business_id'
test_1 = test_1.merge(busi_num, how='inner', on='business_id', suffixes=('_review', '_busi')) # join 'test' and 'busi_num' on 'business_id'

X_train, y_train = train_1.drop(['user_id','business_id','stars_review'], axis=1), train_1.stars_review
X_test, y_test = test_1.drop(['user_id','business_id','stars_review'], axis=1), test_1.stars_review
lasso = Pipeline([('scaler', StandardScaler()),('lasso', Lasso(alpha=0.0015, max_iter=1000, selection='random'))])
lasso.fit(X_train, y_train)
pred_test = lasso.predict(X_test)

# save the predicted ratings to file
with open('rating_predict_content_testset.pkl','wb') as f:
    pickle.dump(pred_test, f)

# free up temporary variable to save memory use
sim_meta_train, sim_pca_train = 0,0
busi_num = 0
train_1, test_1, train_2, test_2 = 0,0,0,0
X_train, y_train, X_test, y_test = 0,0,0,0
pred_train, pred_test = 0,0


#Evaluate content-based personalized recommendation rankings based on different ranking strategies

# function for computing normalized Discounted Cumulative Gain as the ranking metric
# inspired by https://gist.github.com/bwhite/3726239

def dcg_at_k(r, k):
    """Score is discounted cumulative gain (dcg)
    Relevance is positive real values, in this case, the review rating ranging 1-5. 
    Discount starts to take effect from the second item in the list, with the weights being 1/log2(rank position+1)
    (Weights being [1.0, 0.6309, 0.5, 0.4307, 0.3869, ...] for the first items in the rank)
    Example from
    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf
    ---
    Args:
        r: a list or numpy 1D array of relevance scores corresponding to the rank order
        k: Number of results to consider (consider only top-k)
    ---
    Returns:
        Discounted cumulative gain
    """
    r = np.asfarray(r)[:min(len(r),k)]  # convert to float-type numpy array and extract only top-k
    if r.size:
        return np.sum(r / np.log2(np.arange(2, r.size + 2)))
    return None

def ndcg_at_k(r, k):
    """Score is normalized discounted cumulative gain (ndcg), normalized by the maximum achievable DCG (Discounted Cumulative Gain)
    ---
    Args:
        Same as dcg_at_k(r,k)
    ---
    Returns:
        Normalized discounted cumulative gain
    """
    dcg_max = dcg_at_k(sorted(r, reverse=True), k) # compute the maximum achievable DCG based on the perfect ranking
    dcg = dcg_at_k(r, k) # compute the actual DCG based on the actual ranking
    if dcg_max == None or dcg == None:
        return None
    return dcg/dcg_max

#Evaluate recommendation ranking by cosine similarity from review-based pca features

# import linear_kernel from sklearn for computing cosine similarity
from sklearn.metrics.pairwise import linear_kernel

# load in the computed similarity score
with open('similarity_pca_train.pkl','rb') as f: 
    sim_pca = pickle.load(f)

sim_pca = sim_pca.set_index('review_id')

# generate recommendation ranking by cosine similarity score in descending order

# first, join review pcafeature-based similarity scores with all reviews in the testset
rec = test.set_index('review_id').join(sim_pca)
# then rank by similarity in descending order
rec = rec.sort_values('similarity_pca_train', ascending=False)

# look at a particular user_id as an example

user_id = '---1lKK3aKOuomHnwAkAow' # user with 11 review ratings available in the testset

# look at recommendation ranking for the user of interest
rec_id = rec[rec.user_id == user_id].set_index('business_id')
print('Ranking by cosine similarity score:\n', rec_id[['stars','similarity_pca_train']])
ndcg = ndcg_at_k(r=rec_id.stars.values, k=10) # look at the NDCG score@top10
print('\nNormalized discounted cumulative gain achieved at top-10 based on testset:\n', ndcg)

%%time
# evaluate NDCG@top10 based on testset

# only look at NDCG scores for users with at least 10 ratings available in the testset in order to evaluate NDCG@top10
rev_count_by_user = test.groupby('user_id').review_id.count()
user_id_of_interest = rev_count_by_user[rev_count_by_user >= 10].index

# compute NDCG score for all users with more than 10 review ratings in the testset
ndcg_scores_1_10 = []
for user_id in tqdm(user_id_of_interest):     
    rec_id = rec[rec.user_id == user_id].set_index('business_id') # look at ranking for the user of interest
    assert len(rec_id) >= 10
    ndcg = ndcg_at_k(r=rec_id.stars.values, k=10) # compute NDCG score@top10
    ndcg_scores_1_10.append(ndcg)

    %%time
# evaluate NDCG@top5 based on testset

# only look at NDCG scores for users with at least 5 ratings available in the testset in order to evaluate NDCG@top5
rev_count_by_user = test.groupby('user_id').review_id.count()
user_id_of_interest = rev_count_by_user[rev_count_by_user >= 5].index

# compute NDCG score for all users with more than 5 review ratings in the testset
ndcg_scores_1_5 = []
for user_id in tqdm(user_id_of_interest):     
    rec_id = rec[rec.user_id == user_id].set_index('business_id') # look at ranking for the user of interest
    assert len(rec_id) >= 5
    ndcg = ndcg_at_k(r=rec_id.stars.values, k=5) # compute NDCG score@top5
    ndcg_scores_1_5.append(ndcg)

    # Evaluate recommendation ranking by cosine similarity from restaurant text metadata-based features


 # load in the computed similarity score
with open('similarity_meta_train.pkl','rb') as f: 
    sim_meta = pickle.load(f)

sim_meta = sim_meta.set_index('review_id')

# generate recommendation ranking by cosine similarity score in descending order

# first, join restaurant text metadata-based similarity scores with all reviews in the testset
rec = test.set_index('review_id').join(sim_meta)
# then rank by similarity in descending order
rec = rec.sort_values('similarity_meta_train', ascending=False)

# look at a particular user_id as an example

user_id = '---1lKK3aKOuomHnwAkAow' # user with 11 review ratings available in the testset

# look at recommendation ranking for the user of interest
rec_id = rec[rec.user_id == user_id].set_index('business_id')
print('Ranking by cosine similarity score:\n', rec_id[['stars','similarity_meta_train']])
ndcg = ndcg_at_k(r=rec_id.stars.values, k=10) # look at the NDCG score@top10
print('\nNormalized discounted cumulative gain achieved at top-10 based on testset:\n', ndcg)


%%time
# evaluate NDCG@top10 based on testset

# only look at NDCG scores for users with at least 10 ratings available in the testset in order to evaluate NDCG@top10
rev_count_by_user = test.groupby('user_id').review_id.count()
user_id_of_interest = rev_count_by_user[rev_count_by_user >= 10].index

# compute NDCG score for all users with more than 10 review ratings in the testset
ndcg_scores_2_10 = []
for user_id in tqdm(user_id_of_interest):     
    rec_id = rec[rec.user_id == user_id].set_index('business_id') # look at ranking for the user of interest
    assert len(rec_id) >= 10
    ndcg = ndcg_at_k(r=rec_id.stars.values, k=10) # compute NDCG score@top10
    ndcg_scores_2_10.append(ndcg)

%%time
# evaluate NDCG@top5 based on testset

# only look at NDCG scores for users with at least 5 ratings available in the testset in order to evaluate NDCG@top5
rev_count_by_user = test.groupby('user_id').review_id.count()
user_id_of_interest = rev_count_by_user[rev_count_by_user >= 5].index

# compute NDCG score for all users with more than 5 review ratings in the testset
ndcg_scores_2_5 = []
for user_id in tqdm(user_id_of_interest):     
    rec_id = rec[rec.user_id == user_id].set_index('business_id') # look at ranking for the user of interest
    assert len(rec_id) >= 5
    ndcg = ndcg_at_k(r=rec_id.stars.values, k=5) # compute NDCG score@top5
    ndcg_scores_2_5.append(ndcg)

# Evaluate recommendation ranking by the predicted ratings from content-based regression model


# load in the computed rating prediction
with open('rating_predict_testset.pkl','rb') as f: 
    rating_predict = pickle.load(f)

rating_predict = pd.Series(rating_predict) # conver to Pandas Series
rating_predict.name = 'rating_predict'

# generate recommendation ranking by the predicted rating in descending order

# first, join predicted rating with all reviews in the testset
rec = pd.concat([test.reset_index(drop=True),rating_predict], axis=1)
# then rank by predicted rating in descending order
rec = rec.sort_values('rating_predict', ascending=False)

# look at a particular user_id as an example

user_id = '---1lKK3aKOuomHnwAkAow' # user with 11 review ratings available in the testset

# look at recommendation ranking for the user of interest
rec_id = rec[rec.user_id == user_id].set_index('business_id')
print('Ranking by cosine similarity score:\n', rec_id[['stars','rating_predict']])
ndcg = ndcg_at_k(r=rec_id.stars.values, k=10) # look at the NDCG score@top10
print('\nNormalized discounted cumulative gain achieved at top-10 based on testset:\n', ndcg)


%%time
# evaluate NDCG@top10 based on testset

# only look at NDCG scores for users with at least 10 ratings available in the testset in order to evaluate NDCG@top10
rev_count_by_user = test.groupby('user_id').review_id.count()
user_id_of_interest = rev_count_by_user[rev_count_by_user >= 10].index

# compute NDCG score for all users with more than 10 review ratings in the testset
ndcg_scores_3_10 = []
for user_id in tqdm(user_id_of_interest):     
    rec_id = rec[rec.user_id == user_id].set_index('business_id') # look at ranking for the user of interest
    assert len(rec_id) >= 10
    ndcg = ndcg_at_k(r=rec_id.stars.values, k=10) # compute NDCG score@top10
    ndcg_scores_3_10.append(ndcg)

%%time
# evaluate NDCG@top5 based on testset

# only look at NDCG scores for users with at least 5 ratings available in the testset in order to evaluate NDCG@top5
rev_count_by_user = test.groupby('user_id').review_id.count()
user_id_of_interest = rev_count_by_user[rev_count_by_user >= 5].index

# compute NDCG score for all users with more than 5 review ratings in the testset
ndcg_scores_3_5 = []
for user_id in tqdm(user_id_of_interest):     
    rec_id = rec[rec.user_id == user_id].set_index('business_id') # look at ranking for the user of interest
    assert len(rec_id) >= 5
    ndcg = ndcg_at_k(r=rec_id.stars.values, k=5) # compute NDCG score@top5
    ndcg_scores_3_5.append(ndcg)

#Compare NDCG scores for the above three different strategies
        
ndcg_scores = [pd.Series(i) for i in [ndcg_scores_1_10, ndcg_scores_2_10, ndcg_scores_3_10, ndcg_scores_1_5, ndcg_scores_2_5, ndcg_scores_3_5]]
ndcg_scores = pd.concat(ndcg_scores, axis=1)
ndcg_scores.columns = ['ndcg@10_similarity_reviewpca','ndcg@10_similarity_textmetadata','ndcg@10_rating_regressionmodel',\
                       'ndcg@5_similarity_reviewpca','ndcg@5_similarity_textmetadata','ndcg@5_rating_regressionmodel']

# save the NDCG scores to file
with open('ndcg_content_based.pkl','wb') as f:
    pickle.dump(ndcg_scores, f)

ndcg_scores.plot(kind='hist', subplots=True);
ndcg_scores.describe()

# free up temparary variables to save memory
sim_pca, sim_meta, rating_predict, rec, rev_count_by_user, user_id_of_interest = 0,0,0,0,0,0
ndcg_scores, ndcg_scores_1_10, ndcg_scores_2_10, ndcg_scores_3_10, ndcg_scores_1_5, ndcg_scores_2_5, ndcg_scores_3_5 = 0,0,0,0,0,0,0
train, test = 0,0

#Implement content-based recommender module
#Extract pcafeatures based on all restaurant-related reviews

%%time

# groupby business_id and concatenate all reviews of the same business together, reviews are separated by '###'
rev_by_rest = review_s.groupby('business_id').agg({'review_id': 'count', 'text': lambda i: '###'.join(i)}).rename(columns={'review_id':'review_count','text':'review_combined'})
rev_by_rest = rev_by_rest.reset_index()

# Tfidf to extract top 1000 features from restaurant reviews
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1000) # limit to top 1000 words
X = vectorizer.fit_transform(rev_by_rest.review_combined)

# convert to a sparse dataframe
rest_revfeature = pd.SparseDataFrame(X)
rest_revfeature = rest_revfeature.set_index(rev_by_rest.business_id)
rest_revfeature.columns = vectorizer.get_feature_names()
rest_revfeature = rest_revfeature.fillna(0) # fill in missing values with 0 to use PCA

# feature reduction with PCA
from sklearn.decomposition import PCA
pca = PCA()
rest_pcafeature = pca.fit_transform(rest_revfeature)
# keep only top 300 PCA components which account for 80% of the variance
rest_pcafeature = pd.DataFrame(rest_pcafeature[:,:300], index=rest_revfeature.index, columns=[str(i) for i in np.arange(1,301)])
rest_pcafeature.columns.name = 'pca_components'

# rescale rest_pcafeature coefficients so all restaurant feature vectors share the same unit length of 1
rest_pcafeature['root_squared_sum'] = rest_pcafeature.apply(lambda row: np.sqrt(sum([i*i for i in row])), axis=1)
rest_pcafeature = rest_pcafeature.drop('root_squared_sum',axis=1).divide(rest_pcafeature.root_squared_sum, axis=0)


#Aggregate user feature vector based on personal review ratings out of the entire restaurant review dataset

%%time

# build user preference profile by compute a weighted sum of the restaurant vectors for all rated restaurants by the user
# with the corresponding weights being the user’s rating of that restaurant
user_pcafeature = pd.merge(review_s[['user_id','business_id','stars']], rest_pcafeature, how='inner',left_on='business_id',right_index=True).drop('business_id', axis=1)
user_pcafeature.loc[:, '1':'300'] = user_pcafeature.loc[:,'1':'300'].multiply(user_pcafeature.stars, axis=0)

# sum up all component coefficients by user_id
user_pcafeature = user_pcafeature.drop('stars', axis=1).groupby('user_id').sum()

# rescale user_pcafeature coefficients so all user feature vectors share the same unit length of 1
user_pcafeature['root_squared_sum'] = user_pcafeature.apply(lambda row: np.sqrt(sum([i*i for i in row])), axis=1)
user_pcafeature = user_pcafeature.drop('root_squared_sum', axis=1).divide(user_pcafeature.root_squared_sum, axis=0)

#Save the final restaurant and user feature vectors for use in the content-based recommender module

# save the user feature vector and restaurant feature vector to file

with open('rest_pcafeature_all.pkl','wb') as f:
    pickle.dump(rest_pcafeature, f)

# to save user_pcafeature, need to write in chucks of size smaller than 2GB due to a bug in Python 3
max_bytes = 2**31 - 1
bytes_out = pickle.dumps(user_pcafeature)
with open('user_pcafeature_all.pkl','wb') as f:
    for idx in range(0, len(bytes_out), max_bytes):
        f.write(bytes_out[idx:idx+max_bytes])

# free up temparary variables to save memory
rev_by_rest, vectorizer, rest_revfeature, rest_pcafeature, user_pcafeature, review_s, business = 0,0,0,0,0,0,0

#Implementation of the content-based recommender module

import os.path
from sklearn.metrics.pairwise import linear_kernel

# the 'business' and 'review' datasets need to be imported for the content-based recommender module to operate
business = pd.read_csv('business_clean.csv')
review = pd.read_csv('review_clean.csv')
# extract a subset of reviews related to restaurants, since we are only interested in restaurant-type business
review_s = review[review.business_id.isin(business.business_id.unique())]
review = 0 # set review=None to free up memory
# adding 'adjusted_score' to the 'business' dataset, which adjusts the restaurnat average star ratings by the number of ratings it has
globe_mean = ((business.stars * business.review_count).sum())/(business.review_count.sum())
k = 22 # set strength k to 22, which is the 50% quantile of the review counts for all businesses
business['adjusted_score'] = (business.review_count * business.stars + k * globe_mean)/(business.review_count + k)


class Recommender:
    
    def __init__(self, n=5, original_score=False):
        """initiate a Recommender object by passing the desired number of recommendations to make, the default number is 10.
        By default, the adjusted score will be used for ranking; To rank by the original average rating of the restaurant, pass original_score=True
        """
        self.n = n # number of recommendations to make, default is 5
        self.original_score = original_score # boolean indicating whether the original average rating or the adjusted score is used
        # initiate a list of column names to display in the recommendation results
        self.column_to_display = ['state','city','name','address','attributes.RestaurantsPriceRange2','cuisine','style','review_count','stars','adjusted_score']
        
        # initiate the list of recommendations to be all the open restaurants from the entire catalog of 'business' dataframe sorted by the score of interest
        if self.original_score:  # set sorting criteria to the originial star rating
            score = 'stars'
        else:  # set sorting criteria to the adjusted score
            score = 'adjusted_score'
        self.recomm = business[business.is_open == 1].sort_values(score, ascending=False) 
        
    def display_recommendation(self):
        """ Display the list of top n recommended restaurants
        """
        if len(self.recomm) == 0:
            print("Sorry, there is no matching recommendations.")
        elif self.n < len(self.recomm):  # display only the top n from the recommendation list
            print("Below is a list of the top {} recommended restaurants for you: ".format(self.n))
            print(self.recomm.iloc[:self.n][self.column_to_display])
        else:  # display all if # of recommendations is less than self.n
            print("Below is a list of the top {} recommended restaurants for you: ".format(len(self.recomm)))
            print(self.recomm[self.column_to_display])


  #------------------------------------------------------------
    # personalized content-based filtering recommender module
    def content(self, user_id=None):
        """Passing of user_id is required if personalized recommendation is desired.
        """
        
        self.user_id = user_id # user_id for personalized recommendation using collaborative filtering 
        if self.user_id is None:
            print("no user_id is provided!")
            return None
        if len(self.user_id) != 22:
            print("invalid user id!")
            return None
        if self.user_id not in review_s.user_id.unique(): # check if previous restaurant rating/review history is available for the user_id of interest
            print("sorry, no personal data available for this user_id yet!")
            return []
        
        # initiate every time the module is called
        self.recomm = business[business.is_open ==1] # start with all open restaurants from the entire 'business' catalog
        self.column_to_display = ['state','city','name','address','attributes.RestaurantsPriceRange2',\
                                  'cuisine','style','review_count','stars','adjusted_score'] # reset the columns to display
        if 'similarity_score' in self.recomm.columns:
            self.recomm.drop('similarity_score', axis=1, inplace=True) # delete the column of 'cosine_similarity' if already present
        
        # load the saved restaurant pca feature vectors
        with open('rest_pcafeature_all.pkl', 'rb') as f:
            rest_pcafeature = pickle.load(f)
              
        # load the saved user pca feature vectors
        max_bytes = 2**31 - 1
        bytes_in = bytearray(0)
        input_size = os.path.getsize('user_pcafeature_all.pkl')
        with open('user_pcafeature_all.pkl','rb') as f: 
            for _ in range(0, input_size, max_bytes):
                bytes_in += f.read(max_bytes)
            user_pcafeature = pickle.loads(bytes_in)
        
        # predict personalized cosine similarity scores for the user_id of interest
        sim_matrix = linear_kernel(user_pcafeature.loc[user_id].values.reshape(1, -1), rest_pcafeature)
        sim_matrix = sim_matrix.flatten()
        sim_matrix = pd.Series(sim_matrix, index = rest_pcafeature.index)
        sim_matrix.name = 'similarity_score'
        
        # pairing the computed cosine similarity score with the business_id by matching the corresponding matrix indices of the business_id
        self.recomm = pd.concat([sim_matrix, self.recomm.set_index('business_id')], axis=1, join='inner').reset_index()
        
        # filter to unrated business_id only by the user_id of interest if a personal history is available      
        busi_rated = review_s[review_s.user_id == self.user_id].business_id.unique()
        self.recomm = self.recomm[~self.recomm.business_id.isin(busi_rated)]
               
        # sort the recommendation by the cosine similarity score in descending order
        self.recomm = self.recomm.sort_values('similarity_score', ascending=False).reset_index(drop=True)
           
        # add 'similarity_score' to the list of columns to display
        self.column_to_display.insert(0, 'similarity_score') 
        
        # display the list of top n recommendations
        self.display_recommendation()
        
        return self.recomm

#Testing of the personalized content-based recommender module


%%time

# time needed to load in the restaurant and user feature vectors

with open('rest_pcafeature_all.pkl', 'rb') as f: # load the saved restaurant pca feature vectors
    rest_pcafeature = pickle.load(f)
            
max_bytes = 2**31 - 1
bytes_in = bytearray(0)
input_size = os.path.getsize('user_pcafeature_all.pkl')
with open('user_pcafeature_all.pkl','rb') as f:  # load the saved user pca feature vectors
    for _ in range(0, input_size, max_bytes):
        bytes_in += f.read(max_bytes)
    user_pcafeature = pickle.loads(bytes_in)

%%time

# initiate a Recommender object
con = Recommender(n=10)

# test0: display only (same as no keywords)
print("------\nresult from test0 (display only): ")
con.display_recommendation()

# test1: no user id input
print("------\nresult from test1 (no user id input): ")
con.content();

# test 2: invalid user id input
print("------\nresult from test2 (invalid user id input): ")
con.content(user_id='928402');


%%time

# test 3: valid user id (no user data)
print("------\nresult from test3 (valid user id --- no user review data): ")
con.content(user_id='-NzChtoNOw706kps82x0Kg');


%%time

# test 4: valid user id (user has only one review)
print("------\nresult from test4 (valid user id --- user has only one review): ")
con.content(user_id='---89pEy_h9PvHwcHNbpyg');


%%time

# test 5: valid user id (user has over 100 reviews)
print("------\nresult from test5 (valid user id --- user has over 100 reviews): ")
con.content(user_id='Ox89nMY8HpT0vxfKGqDPdA');





